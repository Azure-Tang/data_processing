{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件目的：\n",
    "1. 输入召回的文本数据，寻找到这些文本在原始文本中的位置\n",
    "2. 扩充召回的文本数据，使得每个召回的文本数据都有一个上下文\n",
    "3. 寻找扩充后的文本数据在tokenized后的文本中的位置\n",
    "4. 输出```[{\"content\": \"\", \"start\": 0, \"end\": 0, \"doc_id\": 0}]```格式的json文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Optional, Any\n",
    "import argparse\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = Path().resolve()\n",
    "book_prompt_dir = curr_dir / \"book_prompt_txt\"  # 切分好的书籍段落\n",
    "retrived_data_dir = curr_dir / \"retrived_paragraph\"  # 召回的段落\n",
    "output_dir = curr_dir / \"processed_data_merged_content_merged\"  # 处理好的数据存放目录\n",
    "tokenized_book_dir = curr_dir / \"tokenized_book_prompt\"  # 分词后的书籍段落\n",
    "\n",
    "start_ignore = 562 # 匹配文件时忽略前面的段落\n",
    "end_ignore = 10 # 匹配文件时忽略后面的段落\n",
    "\n",
    "\n",
    "# check if the directory exists\n",
    "if not book_prompt_dir.exists():\n",
    "    print(\"The directory book_prompt_txt does not exist\")\n",
    "if not retrived_data_dir.exists():\n",
    "    print(\"The directory retrived_paragraph does not exist\")\n",
    "if not output_dir.exists():\n",
    "    print(\"The directory processed_data does not exist\")\n",
    "if not tokenized_book_dir.exists():\n",
    "    print(\"The directory tokenized_book_prompt does not exist\")\n",
    "\n",
    "json_template = [{\"content\": \"\", \"start\": 0, \"end\": 0, \"doc_id\": 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_retrived_file(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = f.read()\n",
    "    # 按\"\\n\\n\"分割\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    # 清理换行符和空字符串元素\n",
    "    data = [x for x in data if x.strip() != \"\" and x != \"\\n\"]\n",
    "    # 去除句子头尾的空格和换行符\n",
    "    data = [x.strip() for x in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_and_expand(target_str, folder_path, expansion_length=1000, start_ignore=0, end_ignore=0):\n",
    "    # 将目标字符串转换为一个正则表达式，允许在其字符之间存在最多为n的空白字符\n",
    "    target_regex = re.compile(\".{0,10}\".join(map(re.escape, target_str)), re.DOTALL)\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    assert len(content) >= start_ignore + end_ignore, f\"File {file_path} is too short\"\n",
    "                    content = content[start_ignore:-end_ignore]\n",
    "                    # 首先尝试直接搜索目标字符串\n",
    "                    index = content.find(target_str)\n",
    "                    # 如果直接搜索失败，则尝试使用正则表达式搜索\n",
    "                    if index == -1:\n",
    "                        match = target_regex.search(content)\n",
    "                        if match:\n",
    "                            index = match.start()\n",
    "                    # 如果找到了匹配项, 无论是直接搜索还是正则搜索\n",
    "                    if index != -1:\n",
    "                        start = max(0, index - expansion_length // 2)\n",
    "                        end = min(\n",
    "                            len(content),\n",
    "                            index + len(target_str) + expansion_length // 2,\n",
    "                        )\n",
    "                        # 找到开始和结束点附近的分隔符，以保持句子完整性\n",
    "                        start = content.rfind(\"\\n\", 0, start)\n",
    "                        if start == -1:\n",
    "                            start = 0\n",
    "                        else:\n",
    "                            start += 1\n",
    "                        end = content.find(\"\\n\", end)\n",
    "                        if end == -1:\n",
    "                            end = len(content) - 1\n",
    "                        expanded_str = content[start:end]\n",
    "                        return expanded_str, file\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Target string not found in any file. {target_str}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def read_lookup_table(file_path):\n",
    "    lookup_table = []\n",
    "    # 定义正则表达式来匹配 idx, token_id, 和可能跨越多行的 token_string\n",
    "    pattern = re.compile(r\"(\\d+),\\s*(\\d+),\\s*'((?:[^']|'(?!$))*)'\", re.MULTILINE)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        # 使用正则表达式匹配所有行\n",
    "        matches = pattern.findall(content)\n",
    "        for match in matches:\n",
    "            idx, token_id, token_string = match\n",
    "            # 替换代表换行符的单引号\n",
    "            token_string = token_string.replace(\"\\n'\", \"\\n\")\n",
    "            lookup_table.append((int(idx), int(token_id), token_string))\n",
    "\n",
    "    return lookup_table\n",
    "\n",
    "def find_text_indices(text, lookup_table, n=20):\n",
    "    # 取巧只匹配段落的前20个和后20个字符来定位整段的位置\n",
    "    # 初始化 start_idx 和 end_idx 为 None\n",
    "    start_idx = end_idx = None\n",
    "    # cat lookup_table[3][:]\n",
    "    cat_text = \"\"\n",
    "    position_idx_record = [] # 记录每个char在lookup_table中的idx\n",
    "    for idx, _, token_string in lookup_table:\n",
    "        cat_text += token_string\n",
    "        position_idx_record+=[idx]*len(token_string)\n",
    "\n",
    "    # 通过头，尾各取20个字符，查找text在cat_text中的位置\n",
    "    start_txt = text[:n]\n",
    "    end_txt = text[-n:]\n",
    "\n",
    "    start_idx = cat_text.find(start_txt)\n",
    "    # 如果直接搜索失败，则尝试使用正则表达式搜索\n",
    "    if start_idx == -1:\n",
    "        target_regex = re.compile(\".{0,10}\".join(map(re.escape, text)), re.DOTALL)\n",
    "        match = target_regex.search(cat_text)\n",
    "        if match:\n",
    "            start_idx = match.start()\n",
    "    \n",
    "    end_idx = cat_text.find(end_txt) + len(end_txt) - 1\n",
    "    # 如果直接搜索失败，则尝试使用正则表达式搜索\n",
    "    if end_idx == -1:\n",
    "        target_regex = re.compile(\".{0,10}\".join(map(re.escape, text)), re.DOTALL)\n",
    "        match = target_regex.search(cat_text)\n",
    "        if match:\n",
    "            end_idx = match.end()\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        start_idx = position_idx_record[start_idx]\n",
    "        end_idx = position_idx_record[end_idx]\n",
    "        return start_idx, end_idx\n",
    "    else:\n",
    "        raise Exception(f\"Failed to find text: {text[:10]}\")\n",
    "\n",
    "\n",
    "def merge_strings(str1: str, str2: str) -> str:\n",
    "    # 情况1: 检查一个字符串是否是另一个字符串的子串\n",
    "    if str1 in str2:\n",
    "        return str2\n",
    "    if str2 in str1:\n",
    "        return str1\n",
    "    \n",
    "    # 情况2: 寻找重叠部分并合并\n",
    "    # 检查 str1 结尾与 str2 开始的重叠\n",
    "    for i in range(1, min(len(str1), len(str2)) + 1):\n",
    "        if str1[-i:] == str2[:i]:\n",
    "            return str1 + str2[i:]\n",
    "    \n",
    "    # 检查 str2 结尾与 str1 开始的重叠\n",
    "    for i in range(1, min(len(str1), len(str2)) + 1):\n",
    "        if str2[-i:] == str1[:i]:\n",
    "            return str2 + str1[i:]\n",
    "    \n",
    "    # 如果没有重叠部分，直接连接两个字符串\n",
    "    return str1 + str2\n",
    "\n",
    "def merge_same_content(data: List[dict]) -> List[dict]:\n",
    "    # 按 doc_id 分组\n",
    "    grouped_data = {}\n",
    "    for item in data:\n",
    "        if item['doc_id'] not in grouped_data:\n",
    "            grouped_data[item['doc_id']] = []\n",
    "        grouped_data[item['doc_id']].append(item)\n",
    "    \n",
    "    result = []\n",
    "    # 对每个 doc_id 分别处理\n",
    "    for doc_id, items in grouped_data.items():\n",
    "        # 根据 start 排序\n",
    "        items.sort(key=lambda x: x['start'])\n",
    "        merged = []\n",
    "        for item in items:\n",
    "            # 如果 merged 为空或者当前 item 与 merged 中最后一个元素没有重叠，则直接添加\n",
    "            if not merged or item['start'] > merged[-1]['end']:\n",
    "                merged.append(item)\n",
    "            else:\n",
    "                # 如果有重叠，合并\n",
    "                merged[-1]['end'] = max(merged[-1]['end'], item['end'])\n",
    "                merged[-1]['length'] = merged[-1]['end'] - merged[-1]['start']\n",
    "                merged[-1]['content'] = merge_strings(merged[-1]['content'], item['content'])\n",
    "        result.extend(merged)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_retrived_data(question_path: Path, store_path: Path, tokenized_book_dir,book_prompt_dir, start_ignore: int = 0, end_ignore: int = 0):\n",
    "    processed_data = []\n",
    "    not_found_sentence = []\n",
    "    unextended_sentence = process_retrived_file(question_path)\n",
    "    for i, sentence in enumerate(unextended_sentence):\n",
    "        # print(f\"processing {i}th sentence\")\n",
    "        # print(sentence[:10])\n",
    "        meta_data = json_template[0].copy()\n",
    "        # 保存并扩充句子\n",
    "        content, doc_id = find_and_expand(sentence, book_prompt_dir, start_ignore=start_ignore, end_ignore=end_ignore)\n",
    "        if content is None:\n",
    "            # raise Exception(f\"Failed to find and expand sentence: {sentence}\")\n",
    "            # print(f\"Failed to find and expand sentence: {sentence}\")\n",
    "            meta_data[\"not_found_content\"] = sentence\n",
    "            continue\n",
    "        # 保存处理后的数据\n",
    "        meta_data[\"content\"] = content\n",
    "        # 保存doc_id int\n",
    "        meta_data[\"doc_id\"] = int(doc_id.split(\".\")[0])\n",
    "\n",
    "        lookup_table = read_lookup_table(tokenized_book_dir / f\"answer_{doc_id}\")\n",
    "        # compiled_regex = build_regex_from_lookup_table(lookup_table)\n",
    "        start_idx, end_idx = find_text_indices(content, lookup_table)\n",
    "        # start_idx, end_idx = find_text_indices(content, lookup_table)\n",
    "        if(start_idx is None or end_idx is None):\n",
    "            not_found_sentence.append(sentence)\n",
    "            continue\n",
    "        meta_data[\"start\"] = start_idx\n",
    "        meta_data[\"end\"] = end_idx + 1 # 左闭右开\n",
    "        meta_data[\"length\"] = end_idx - start_idx\n",
    "        processed_data.append(meta_data)\n",
    "\n",
    "    processed_data = merge_same_content(processed_data)\n",
    "\n",
    "    json.dump(\n",
    "        processed_data,\n",
    "        store_path.open(\"w\", encoding=\"utf-8\"),\n",
    "        ensure_ascii=False,\n",
    "        indent=4,\n",
    "    )\n",
    "    json.dump(\n",
    "        not_found_sentence,\n",
    "        store_path.with_suffix(\".not_found.json\").open(\"w\", encoding=\"utf-8\"),\n",
    "        ensure_ascii=False,\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    process_retrived_data(retrived_data_dir / f\"{i}.txt\", output_dir / f\"{i}.json\", tokenized_book_dir, book_prompt_dir, start_ignore=start_ignore, end_ignore=end_ignore)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
